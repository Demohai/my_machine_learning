{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data_set/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data_set/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data_set/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data_set/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./MNIST_data_set/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train set x is: (55000, 784)\n",
      "The shape of train set y is: (55000, 10)\n",
      "The shape of validation set x is: (5000, 784)\n",
      "The shape of validation set y is: (5000, 10)\n",
      "The shape of test set x is: (10000, 784)\n",
      "The shape of test set y is: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of train set x is:\", mnist.train.images.shape)\n",
    "print(\"The shape of train set y is:\", mnist.train.labels.shape)\n",
    "print(\"The shape of validation set x is:\", mnist.validation.images.shape)\n",
    "print(\"The shape of validation set y is:\", mnist.validation.labels.shape)\n",
    "print(\"The shape of test set x is:\", mnist.test.images.shape)\n",
    "print(\"The shape of test set y is:\", mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输入数据函数\n",
    "def input_data(Input_node, Output_node):\n",
    "    x = tf.placeholder(tf.float32, shape=[None,Input_node], name='input_x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None,Output_node], name='output_y')\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 参数初始化\n",
    "def parameter_initialize(Input_node, Layer1_node, Output_node):\n",
    "    W1 = tf.Variable(tf.truncated_normal([Input_node, Layer1_node], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[Layer1_node]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([Layer1_node, Output_node], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.constant(0.1, shape=[Output_node]))\n",
    "    parameter = {\"W1\":W1,\"b1\":b1,\n",
    "                 \"W2\":W2,\"b2\":b2}\n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 前向过程\n",
    "def inference(input_tensor, parameter):\n",
    "    W1 = parameter[\"W1\"]\n",
    "    b1 = parameter[\"b1\"]\n",
    "    W2 = parameter[\"W2\"]\n",
    "    b2 = parameter[\"b2\"]\n",
    "    layer1 = tf.nn.relu(tf.matmul(input_tensor, W1) + b1)\n",
    "    return tf.nn.relu(tf.matmul(layer1, W2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义损失函数 \n",
    "def loss(y_pred, y, parameter):\n",
    "    W1 = parameter[\"W1\"]\n",
    "    W2 = parameter[\"W2\"]\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_pred, labels=tf.argmax(y,1)))\n",
    "    # 定义L2正则项\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(regularization_rate)\n",
    "    regularition = regularizer(W1) + regularizer(W2)\n",
    "    # 交叉熵与L2正则项相加，作为损失函数\n",
    "    cost = cross_entropy + regularition\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(Learning_rate_base, Learning_rate_decay, mini_batch_size, cost):\n",
    "    ## 学习率指数衰减函数\n",
    "    ## tf.train.exponential_decay(learning_rate_base,  global_step,  decay_steps,  learning_rate_decay,  staircase=False or True,  name(optional))\n",
    "    ## decayed_learning_rate = learning_rate_base *learning_rate_decay^ (global_step/decay_steps)\n",
    "    ## 如果staircase=True,global_step/decay_steps为整除；如果staircase=True,global_step/decay_steps为正常除\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(Learning_rate_base, global_step, mnist.train.num_examples / mini_batch_size, learning_rate_decay, staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost,global_step=global_step)\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义评估函数，此程序以准确度作为评估指标\n",
    "def evaluate(y, y_pred):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_pred,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建mini_batch函数，将训练集划分成若干个mini_batch_size大小的子集\n",
    "\n",
    "def random_mini_batches(x, y, mini_batch_size):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    m = x.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_x = x[permutation, :]\n",
    "    shuffled_y = y[permutation, :]\n",
    "\n",
    "    # Step 2: Partition (shuffled_x, shuffled_y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_x = shuffled_x[k*mini_batch_size : (k+1)*mini_batch_size, :]\n",
    "        mini_batch_y = shuffled_y[k*mini_batch_size : (k+1)*mini_batch_size, :]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_x, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_x = shuffled_x[num_complete_minibatches*mini_batch_size : m, :]\n",
    "        mini_batch_y = shuffled_y[num_complete_minibatches*mini_batch_size : m, :]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_x, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(Input_node, Layer1_model, Output_node, Learning_rate_base, Learning_rate_decay):\n",
    "    x, y = input_data(Input_node, Output_node)\n",
    "    parameter = parameter_initialize(Input_node, Layer1_node, Output_node)\n",
    "    y_pred = inference(x, parameter)\n",
    "    cost = loss(y_pred, y, parameter)\n",
    "    train_step = train(Learning_rate_base, Learning_rate_decay, mini_batch_size, cost)\n",
    "    accuracy = evaluate(y, y_pred)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        validate_feed = {x: mnist.validation.images, y: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y: mnist.test.labels}\n",
    "        \n",
    "\n",
    "        # 遍历整个训练集多次\n",
    "        for i in range(training_steps):\n",
    "            # 每遍历一次训练集，计算验证集的准确度\n",
    "            validate_accuracy = sess.run(accuracy, feed_dict=validate_feed)\n",
    "            print(\"After %d training steps ,validation accuracy is %g\" % (i, validate_accuracy))\n",
    "            minibatches = random_mini_batches(mnist.train.images, mnist.train.labels, mini_batch_size)\n",
    "            # 遍历整个训练集一次\n",
    "            for minibatch in minibatches:\n",
    "                (x_batch, y_batch) = minibatch\n",
    "                sess.run(train_step, feed_dict={x: x_batch, y: y_batch})\n",
    "        \n",
    "        test_accuracy = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"After %d training steps ,test accuracy is %g\" % (training_steps, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps ,validation accuracy is 0.0842\n",
      "After 1 training steps ,validation accuracy is 0.934\n",
      "After 2 training steps ,validation accuracy is 0.9552\n",
      "After 3 training steps ,validation accuracy is 0.963\n",
      "After 4 training steps ,validation accuracy is 0.9684\n",
      "After 5 training steps ,validation accuracy is 0.9676\n",
      "After 6 training steps ,validation accuracy is 0.972\n",
      "After 7 training steps ,validation accuracy is 0.9742\n",
      "After 8 training steps ,validation accuracy is 0.9716\n",
      "After 9 training steps ,validation accuracy is 0.9742\n",
      "After 10 training steps ,validation accuracy is 0.976\n",
      "After 11 training steps ,validation accuracy is 0.977\n",
      "After 12 training steps ,validation accuracy is 0.9766\n",
      "After 13 training steps ,validation accuracy is 0.976\n",
      "After 14 training steps ,validation accuracy is 0.9782\n",
      "After 15 training steps ,validation accuracy is 0.9774\n",
      "After 16 training steps ,validation accuracy is 0.976\n",
      "After 17 training steps ,validation accuracy is 0.9776\n",
      "After 18 training steps ,validation accuracy is 0.9788\n",
      "After 19 training steps ,validation accuracy is 0.9768\n",
      "After 20 training steps ,test accuracy is 0.9787\n"
     ]
    }
   ],
   "source": [
    "# 定义输入层，隐藏层和输出层的节点数量\n",
    "Input_node = 784\n",
    "Layer1_node = 500\n",
    "Output_node = 10\n",
    "# 定义每一训练批次的样本数量\n",
    "mini_batch_size = 64\n",
    "# 模型相关参数\n",
    "learning_rate_base = 0.1      #基础学习率\n",
    "learning_rate_decay = 0.99  #学习率衰减因子\n",
    "regularization_rate = 0.001  #L2正则化因子\n",
    "training_steps = 20       #遍历整个训练集的次数\n",
    "model(Input_node, Layer1_node, Output_node, learning_rate_base, learning_rate_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
