{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 卷积神经网络例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用tensorflow创建一个卷积神经网络\n",
    "\n",
    "·作者：朱海\n",
    "\n",
    "·项目：https://github.com/Demohai/my_tensorflow_learn/tree/master/Neural_Network/convolutional_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data_set/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data_set/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data_set/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data_set/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 导入MNIST数据\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data_set/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train set x is: (55000, 784)\n",
      "The shape of train set y is: (55000, 10)\n",
      "The shape of validation set x is: (5000, 784)\n",
      "The shape of validation set y is: (5000, 10)\n",
      "The shape of test set x is: (10000, 784)\n",
      "The shape of test set y is: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of train set x is:\", mnist.train.images.shape)\n",
    "print(\"The shape of train set y is:\", mnist.train.labels.shape)\n",
    "print(\"The shape of validation set x is:\", mnist.validation.images.shape)\n",
    "print(\"The shape of validation set y is:\", mnist.validation.labels.shape)\n",
    "print(\"The shape of test set x is:\", mnist.test.images.shape)\n",
    "print(\"The shape of test set y is:\", mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 神经网络参数\n",
    "num_input = 784   #MNist数据输入为28*28 = 784\n",
    "num_class = 10  #手写字母共有九个(0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义输入函数\n",
    "def data_input(num_input, num_class):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, num_input])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, num_class])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    return x, y, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建简单的卷积层\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.add(x, b)       \n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建简单的池化层\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 参数W和b初始化函数\n",
    "def parameters_initialize():\n",
    "    # filter:5x5, 1 input, 32 outputs\n",
    "    WC1 = tf.Variable(tf.random_normal([5, 5, 1, 32])) #随机正态分布\n",
    "    bc1 = tf.Variable(tf.random_normal([32]))\n",
    "    # filter:5x5, 32 inputs, 64 outputs\n",
    "    WC2 = tf.Variable(tf.random_normal([5, 5, 32, 64]))\n",
    "    bc2 = tf.Variable(tf.random_normal([64]))\n",
    "    # fully connected: 7*7*64 inputs, 1024 outputs\n",
    "    WD1 = tf.Variable(tf.random_normal([7*7*64, 1024])) #经过两个padding='SAME'的卷积层，和两个(2, 2)的MaxPool后，输出变为(7,7,64)\n",
    "    bd1 = tf.Variable(tf.random_normal([1024]))\n",
    "    # fully connected: 1024 inputs, 10 outputs\n",
    "    WD2 = tf.Variable(tf.random_normal([1024, 10]))\n",
    "    bd2 = tf.Variable(tf.random_normal([10]))\n",
    "    \n",
    "    weights = {\"WC1\": WC1, \"WC2\": WC2, \"WD1\": WD1, \"WD2\": WD2}\n",
    "    bias = {\"bc1\": bc1, \"bc2\": bc2, \"bd1\": bd1, \"bd2\": bd2}\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建卷积神经网络模型\n",
    "def conv_net(x, weights, bias, keep_prob):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    # Convolution Layer1\n",
    "    WC1 = weights[\"WC1\"]\n",
    "    bc1 = bias[\"bc1\"]\n",
    "    conv1 = conv2d(x, WC1, bc1)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    # Convolution Layer2\n",
    "    WC2 = weights[\"WC2\"]\n",
    "    bc2 = bias[\"bc2\"]\n",
    "    conv2 = conv2d(conv1, WC2, bc2)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Fully Connected Layer1\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    WD1 = weights[\"WD1\"]\n",
    "    bd1 = bias[\"bd1\"]\n",
    "    fc1 = tf.reshape(conv2, [-1, WD1.get_shape().as_list()[0]])\n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(fc1, WD1), bd1)  # 使用tf.matmul时，注意两个矩阵相乘的顺序，否则会造成输出矩阵形状有误\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    # Fully Connected Layer2(Output)\n",
    "    WD2 = weights[\"WD2\"]\n",
    "    bd2 = bias[\"bd2\"]\n",
    "    fc2 = tf.add(tf.matmul(fc1, WD2), bd2)\n",
    "    \n",
    "    \n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(num_input=784, num_class=10, learning_rate=0.001, num_steps=200, batch_size=128):\n",
    "    x, y, keep_prob = data_input(num_input, num_class)\n",
    "    weights, bias = parameters_initialize()\n",
    "    fc2 = conv_net(x, weights, bias, keep_prob)\n",
    "    y_pred = tf.nn.softmax(fc2)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc2, labels=y)) \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start training\n",
    "    with tf.Session() as sess:\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "        \n",
    "        for i in range(num_steps):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            if i % 10 == 0:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "                print(\"Step \" + str(i) + \", Minibatch Loss= \" + \\\n",
    "                    \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                    \"{:.3f}\".format(acc))\n",
    "            # Run optimization op(backprop)\n",
    "            sess.run(train_op, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.8})\n",
    "        print(\"Optimization finished\")\n",
    "        \n",
    "        # Calculate accuracy for  MNIST test images\n",
    "        print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Minibatch Loss= 154981.9375, Training Accuracy= 0.125\n",
      "Step 10, Minibatch Loss= 41111.5664, Training Accuracy= 0.195\n",
      "Step 20, Minibatch Loss= 15696.7266, Training Accuracy= 0.414\n",
      "Step 30, Minibatch Loss= 10589.9141, Training Accuracy= 0.555\n",
      "Step 40, Minibatch Loss= 6577.9272, Training Accuracy= 0.734\n",
      "Step 50, Minibatch Loss= 4819.0166, Training Accuracy= 0.805\n",
      "Step 60, Minibatch Loss= 5597.4814, Training Accuracy= 0.711\n",
      "Step 70, Minibatch Loss= 2081.1616, Training Accuracy= 0.852\n",
      "Step 80, Minibatch Loss= 3917.0784, Training Accuracy= 0.805\n",
      "Step 90, Minibatch Loss= 3314.1797, Training Accuracy= 0.797\n",
      "Step 100, Minibatch Loss= 1136.6399, Training Accuracy= 0.898\n",
      "Step 110, Minibatch Loss= 2390.5046, Training Accuracy= 0.914\n",
      "Step 120, Minibatch Loss= 2496.7153, Training Accuracy= 0.859\n",
      "Step 130, Minibatch Loss= 2777.8389, Training Accuracy= 0.859\n",
      "Step 140, Minibatch Loss= 2528.3164, Training Accuracy= 0.883\n",
      "Step 150, Minibatch Loss= 1969.7141, Training Accuracy= 0.906\n",
      "Step 160, Minibatch Loss= 2022.6738, Training Accuracy= 0.859\n",
      "Step 170, Minibatch Loss= 2552.0134, Training Accuracy= 0.891\n",
      "Step 180, Minibatch Loss= 574.5723, Training Accuracy= 0.969\n",
      "Step 190, Minibatch Loss= 1728.4672, Training Accuracy= 0.930\n",
      "Optimization finished\n",
      "Testing Accuracy: 0.9186\n"
     ]
    }
   ],
   "source": [
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
