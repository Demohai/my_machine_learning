{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data_set/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data_set/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data_set/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data_set/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 导入MNIST数据\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data_set/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train set x is: (55000, 784)\n",
      "The shape of train set y is: (55000, 10)\n",
      "The shape of validation set x is: (5000, 784)\n",
      "The shape of validation set y is: (5000, 10)\n",
      "The shape of test set x is: (10000, 784)\n",
      "The shape of test set y is: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of train set x is:\", mnist.train.images.shape)\n",
    "print(\"The shape of train set y is:\", mnist.train.labels.shape)\n",
    "print(\"The shape of validation set x is:\", mnist.validation.images.shape)\n",
    "print(\"The shape of validation set y is:\", mnist.validation.labels.shape)\n",
    "print(\"The shape of test set x is:\", mnist.test.images.shape)\n",
    "print(\"The shape of test set y is:\", mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 神经网络参数\n",
    "num_input = 784   #MNist数据输入为28*28 = 784\n",
    "num_class = 10  #手写字母共有九个(0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义输入函数\n",
    "def data_input(num_input, num_class):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, num_input])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, num_class])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    return x, y, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建简单的卷积层\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.add(x, b)       \n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建简单的池化层\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 参数W和b初始化函数\n",
    "def parameters_initialize():\n",
    "    # filter:5x5, 1 input, 32 outputs\n",
    "    WC1 = tf.Variable(tf.random_normal([5, 5, 1, 32])) #随机正态分布\n",
    "    bc1 = tf.Variable(tf.random_normal([32]))\n",
    "    # filter:5x5, 32 inputs, 64 outputs\n",
    "    WC2 = tf.Variable(tf.random_normal([5, 5, 32, 64]))\n",
    "    bc2 = tf.Variable(tf.random_normal([64]))\n",
    "    # fully connected: 7*7*64 inputs, 1024 outputs\n",
    "    WD1 = tf.Variable(tf.random_normal([7*7*64, 1024])) #经过两个padding='SAME'的卷积层，和两个(2, 2)的MaxPool后，输出变为(7,7,64)\n",
    "    bd1 = tf.Variable(tf.random_normal([1024]))\n",
    "    # fully connected: 1024 inputs, 10 outputs\n",
    "    WD2 = tf.Variable(tf.random_normal([1024, 10]))\n",
    "    bd2 = tf.Variable(tf.random_normal([10]))\n",
    "    \n",
    "    weights = {\"WC1\": WC1, \"WC2\": WC2, \"WD1\": WD1, \"WD2\": WD2}\n",
    "    bias = {\"bc1\": bc1, \"bc2\": bc2, \"bd1\": bd1, \"bd2\": bd2}\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建卷积神经网络模型\n",
    "def conv_net(x, weights, bias, keep_prob):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    # Convolution Layer1\n",
    "    WC1 = weights[\"WC1\"]\n",
    "    bc1 = bias[\"bc1\"]\n",
    "    conv1 = conv2d(x, WC1, bc1)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    # Convolution Layer2\n",
    "    WC2 = weights[\"WC2\"]\n",
    "    bc2 = bias[\"bc2\"]\n",
    "    conv2 = conv2d(conv1, WC2, bc2)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Fully Connected Layer1\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    WD1 = weights[\"WD1\"]\n",
    "    bd1 = bias[\"bd1\"]\n",
    "    fc1 = tf.reshape(conv2, [-1, WD1.get_shape().as_list()[0]])\n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(fc1, WD1), bd1)  # 使用tf.matmul时，注意两个矩阵相乘的顺序，否则会造成输出矩阵形状有误\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    # Fully Connected Layer2(Output)\n",
    "    WD2 = weights[\"WD2\"]\n",
    "    bd2 = bias[\"bd2\"]\n",
    "    fc2 = tf.add(tf.matmul(fc1, WD2), bd2)\n",
    "    \n",
    "    \n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(num_input=784, num_class=10, learning_rate=0.001, num_steps=200, batch_size=128):\n",
    "    x, y, keep_prob = data_input(num_input, num_class)\n",
    "    weights, bias = parameters_initialize()\n",
    "    fc2 = conv_net(x, weights, bias, keep_prob)\n",
    "    y_pred = tf.nn.softmax(fc2)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc2, labels=y)) \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start training\n",
    "    with tf.Session() as sess:\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "        \n",
    "        for i in range(num_steps):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            if i % 10 == 0:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "                print(\"Step \" + str(i) + \", Minibatch Loss= \" + \\\n",
    "                    \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                    \"{:.3f}\".format(acc))\n",
    "            # Run optimization op(backprop)\n",
    "            sess.run(train_op, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.8})\n",
    "        print(\"Optimization finished\")\n",
    "        \n",
    "        # Calculate accuracy for  MNIST test images\n",
    "        print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Minibatch Loss= 86606.3125, Training Accuracy= 0.078\n",
      "Step 10, Minibatch Loss= 24764.9609, Training Accuracy= 0.250\n",
      "Step 20, Minibatch Loss= 8558.9395, Training Accuracy= 0.523\n",
      "Step 30, Minibatch Loss= 6962.2246, Training Accuracy= 0.672\n",
      "Step 40, Minibatch Loss= 5631.1211, Training Accuracy= 0.734\n",
      "Step 50, Minibatch Loss= 3139.0696, Training Accuracy= 0.805\n",
      "Step 60, Minibatch Loss= 2904.9099, Training Accuracy= 0.852\n",
      "Step 70, Minibatch Loss= 2031.6489, Training Accuracy= 0.898\n",
      "Step 80, Minibatch Loss= 1246.9424, Training Accuracy= 0.898\n",
      "Step 90, Minibatch Loss= 2153.9624, Training Accuracy= 0.859\n",
      "Step 100, Minibatch Loss= 704.1632, Training Accuracy= 0.945\n",
      "Step 110, Minibatch Loss= 1561.0176, Training Accuracy= 0.875\n",
      "Step 120, Minibatch Loss= 3410.7646, Training Accuracy= 0.852\n",
      "Step 130, Minibatch Loss= 458.5975, Training Accuracy= 0.938\n",
      "Step 140, Minibatch Loss= 1809.9033, Training Accuracy= 0.883\n",
      "Step 150, Minibatch Loss= 577.9481, Training Accuracy= 0.914\n",
      "Step 160, Minibatch Loss= 1872.1642, Training Accuracy= 0.930\n",
      "Step 170, Minibatch Loss= 1443.5369, Training Accuracy= 0.938\n",
      "Step 180, Minibatch Loss= 937.4452, Training Accuracy= 0.930\n",
      "Step 190, Minibatch Loss= 1497.2415, Training Accuracy= 0.906\n",
      "Optimization finished\n",
      "Testing Accuracy: 0.9265\n"
     ]
    }
   ],
   "source": [
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
